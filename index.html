<!doctype html>
<html lang="en">
<head>
<title>Virtual Try-on (VTON)</title>
<meta property="og:title" content="Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Virtual Try-on (VTON)</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>Style-based Global Appearance Flow For Virtual Try-On</h2>
</div>
</div>
<div class="row">
<div class="col">

<h2>Motivation</h2>

<p>
 Doing our internship in retail companies (Amazon and Walmart) this summer, 
  made us think alike and want to go deeper into computer vision and deep learning applications in retail. 
  We thought of working on improving Virtual Mirrors to enhance customer personalization and experience. 
  Virtual mirrors are becoming the central focus of personalization and customer experience enhancement in retail. 
  It is basically a traditional mirror with a display behind the glass. Powered with computer vision cameras and AR, 
  these mirrors can display a broad range of contextual information, which, in turn, helps buyers connect with the brand better. 
  This ML-based engine provides its users with real-time fashion recommendations by observing their current outfits.

</p>
<h2>Abstract</h2>
<p>
  Image-based virtual try-on aims to fit an in-shop garments into a clothed person image. To achieve this, 
  a key step is garment warping which spatially aligns the target garment with the corresponding body parts in 
  the person image. Prior methods typically adopt a local appearance flow estimation model. They are thus 
  intrinsically susceptible to difficult body poses/occlusions and large mis-alignments between person and garment images. 
  To overcome this limitation, a novel global appearance flow estimation model is proposed in this work. 
  For the first time, a StyleGAN based architecture is adopted for appearance flow estimation. 
  This enables us to take advantage of a global style vector to encode a whole-image context to cope with the aforementioned 
  challenges. To guide the StyleGAN flow generator to pay more attention to local garment deformation, a flow refinement 
  module is introduced to add local context. A VTON model aims to fit an in-shop garment into a person's image. A key 
  objective of a VTON model is to align the in-shop garment with the corresponding body parts in the person image. 
  This is due to the fact that the in-shop garment is usually not spatially aligned with the person's image. Without the 
  spatial alignment, directly applying advanced detail-preserving image to image translation models to fuse the texture in 
  person image and garment image will result in unrealistic effect in the generated try-on image, especially in the occluded
   and misaligned regions.
</p>
<h2>About Models</h2>
<p>
  Given the clothing-agnostic person representation p and the target clothing image c, 
  we propose to synthesize the reference image I through reconstruction such that a natural transfer from c to the 
  corresponding region of p can be learned. In particular, we utilize a multi-task encoder decoder framework that generates a clothed person image along with a clothing mask of the person as well. 
  In addition to guiding the network to focus on the clothing region, the predicted clothing mask will be 
  further utilized to refine the generated result. The encoder-decoder is a general type of U-net architecture with skip 
  connections to directly share information between layers through bypassing connections.
</p>
<p>
The refinement network in VITON is trained to render the coarse blurry region leveraging realistic details from a deformed target item. The network will be a fully convolutional model.
  
</p>
<p>
  To achieve other goals like pair recommendation and fill in the blank we plan to use Conditional Analogy GAN (CAGAN). CAGAN formulates the virtual try-on task as an image analogy problem - it treats the original item and the target clothing item together as a condition when training a Cycle-GAN. 

</p>

<h2>About Data</h2>
<p>
  We plan to experiment our model on the Zalando dataset. It contains a training set containing 14,221 image pairs and a testing dataset of 2,032. Each pair means a person's image and the image of the garment on the person. Both person and garment images are of the resolution 256 x 192. 
  We also plan to create a testing dataset, denoted by augmented VITON, to evaluate model's robustness to the random positioned person image with larger misalignments with the garment images in the original dataset. We are also finding some more freely available datasets so that we can increase 
  robustness of our model but finding such a 
  dataset is difficult as its a service provided by various companies with a hefty cost. This topic is still a research project incorporated by various companies such as C3.AI, Nvidia, PathAI, Snapchat, Amazon etc. 
</p>

<h2>Literature Survey</h2>
<p>
  With the advancement in eCommerce, clothing has been a highly revenue-generating field.
   There numerous types of research & experiments have been proposed and tested for customer 
   satisfaction with optimizing cost. About a decade ago, a virtual try-on was incorporated by the concept of parsing clothes, 
   an idea similar to annotations in machine learning. Retrieving similar styles to parse clothing items. 
   clothes parsing Yamaguchi, et al (2013) <a href="#Paper doll parsing"> [2]</a>, clothing seen on the street to online products S. Liu, et al (2012)  <a href="#Street-toshop"> [3]</a>
   fashion recommendation Y. Hu, et al (2015) <a href="#Collaborative fashion recommendation">[2]</a>, visual 
   compatibility learning <a href="#Learning fashion">[7]</a> and fashion trend prediction. A year later, making use of parsing technologies, 
   a trend of recommendation shot up customer interest <a href="#Collaborative fashion recommendation">[3]</a>. Then with AR technologies and integration with 3D visuals, 
   virtual try-on became realistic with body shape and size. 
</p>
<p>
  GANs <a href="#Generative Adversarial Nets">[6]</a> showed promising advancement with their realistic generative results. Class labels,
priorly done by image parsing helped in generating clothes with desired properties <a href="#Conditional image synthesis">[7]</a>. 
Labels also served as conditions in GANs. This idea was similar to the image-to-image translation using conditional GANs and 
it became the root of virtual-try-on. Image-to-image translation not only transformed the image into the desired output, 
i.e. fitting of clothes, sleeves length, cloth texture/material, etc but it also allowed training a CNN using a 
regression loss as an alternative to GANs for this task without adversarial training. 
These methods can produce photo-realistic images but have limited success when geometric changes occur <a href="#9">[9]</a>. 
GANs are computationally expensive, as high graphic computing is required to generated precise and realistic image. 

</p>
<p>
  Resembled with the listed set of work, our concentration will be 2D images only. 
  It will be challenging compared to contemporary work, which takes 3D input variables. 
  From past implementations that focus on tweaking attributes(color and textures) of apparel <a href="#10">[10]</a> 
  for interactive search we intend to improve with the VITON model. In the context of image-to-image translation 
  for fashion applications, the main drawback of Yoo et al <a href="#Pixellevel domain transfer">[5]</a>  was the transformation of a clothed person conditioned 
  on a product image and backward. It ignored the condition of the model's pose. of the person's pose.  
  Lassner et al. <a href="#11">[11]</a> proposed a model that lacked conditions/control on fashion items in the generated output.
</p>
<p>
  FashionGAN <a href="#12">[12]</a>, substituted a category of cloth on a model with a new one specified by text descriptions.
   However, we are curious about the accurate replacement of the clothing item in a reference image with a target model 
   in the image. Additionally, there are various Virtual try-on implementation attempted conducted in computer graphics as well.
    Guan et al. didn't have Yoo's drawback in his proposed DRAPE <a href="#13">[13]</a> to simulate 2D clothes on 3D bodies in different shapes
     and poses. Sekine et al. <a href="#14">[14]</a> presented a VFS (Virtual Fitting System) <a href="#14">[14]</a> which perfectly adjusted 2D clothing 
     images on consumers by inferring. This was implemented using image depth analysis and still required high computation. 
     Yang et al. <a href="#15">[15]</a>  recovered a 3D mesh of the garment from a single view 2D image, which is further re-targeted to other human bodies. 
</p>
<p>
  There hasn't been much research done in computer vision to examine the virtual try-on problem. 
  A conditional analogy GAN to exchange fashion items was recently presented by Jetchev and Bergmann <a href="#16">[16]</a>. 
  However, they need product photos of both the target item and the original item on the individual during testing, 
  which makes it impractical in real-world situations. Furthermore, without any human representing or explicitly 
  considering deformations, it fails to provide photo-realistic virtual try-on results. Thus we focus on creating an 
  accurate photo-realistic image straight from 2D photographs, which is more computationally economical, as opposed to 
  relying on 3D measurements to accomplish perfect clothing simulation. 

</p>
<h2>Major Challenges</h2>
<h3>Fashion Item Representation: </h3>
<p>
  Traditional recommender systems such as Collaborative Filtering or Content-Based Filtering have difficulties in the fashion domain due to the sparsity of purchase data, or the insufficient detail about the visual appearance of the product in category names. Instead, more recent literature has leveraged models that capture a rich representation of fashion items through product images, text descriptions or customer reviews, or videos which are often learned through surrogate tasks like classification or product retrieval. However, learning product representations from such input data requires large datasets to generalize well across different image (or text) styles, attribute variations, etc.
</p>

<h3>Fashion Item Compatibility: </h3>
<p>
  Training a model that is able to predict if two fashion items 'go together', or directly combine several products into an outfit, is a challenging task. Different item compatibility signals studied in recent literature include co-purchase data, outfits composed by professional fashion designers, or combinations found by 
  analyzing what people wear in social media pictures. From this compatibility information, associated image and text data are then used to learn to generalize to stylistically similar products. 
</p>

<h3>Personalization and Fit: </h3>
<p>
  The best fashion product to recommend depends on factors such as the location where the outfit will be used, the season or occasion, or the cultural and social background of the customer. A challenging task in fashion recommendation systems is how discovering and integrating these disparate factors. In addition to predicting what size of a product will be more comfortable to wear, body shape can influence stylistic choices. 
</p>

<h3>Discovering Trends: </h3>
<p>
  Being able to forecast consumer preferences is valuable for fashion designers and retailers in order to optimize product-to-market fit, logistics, and advertising. Many factors are confounded in what features are considered 'fashionable' or 'trendy', like seasonality, geographical influence, historical events, or style dynamics. 
</p>

<h2>Potential Contributions</h2> 
<h6>In the following, we present the most important goals we have identified in the literature. 
  We plan to solve some of these in the due course of time.</h6>
<h3> Goal 1 - Outfit Generation: </h3>
<p> The goal of this task is, given a fashion item 'xq' (e.g., skirt) representing the user's current interest, find the best item x belgongs I (e.g., shirt) or the fashion outfit F belongs I (e.g., shirt, pants, hat) that goes/go well with the input query.</p>

<h3> Goal 2 - Outfit Recommendation </h3>
<p> 
  This goal is related to the fashion and outfit recommendation task, where a set of objects is 
  recommended to the user at once, by maximizing a utility function that measures the suitability of 
  recommending a fashion outfit to a specific user. 
</p>
<h3> Goal 3 - Pair recommendation: </h3>
<p> 
  This goal is a simplification of the outfit recommendation goal when N = 2. This is typically performed as a top-bottom or bottom-up recommendation. 
  In this task, given clothes related to the upper part of the body, the aim is to predict 
  the possible lower part and vice versa. For instance, a user has a cloth (e.g., tops) related to the upper part of the body and look for the bottom 
  (e.g., trousers or skirts) from a large collection that best match the tops. This typically requires a collection of pairs of top and bottom images for
  compatibility modeling. </p>
<h3> Goal 4 - Fill In the Blank (FITB): </h3>
<p>
  This goal is used in a setting where we are given an incomplete outfit F - (e.g., shirt, pants, accessories) with a missing item (e.g., shoes), and the method must find the best missing fashion item x belongs to {x1, x2, x3, x4} 
  from multiple choices where F = {F -, x} has fashion items, which are compatible visually. This is a convenient scenario in real life, e.g., a user wants to choose a pair of shoes to suit his pants and jacket. </p>

<h3>References</h3>
<p><a name="A Review">[1]</a> <a href="https://arxiv.org/pdf/2202.02757.pdf">A Review of Modern Fashion Recommender Systems</a>
</p>
<p><a name="Style-Based">[2]</a> <a href="https://arxiv.org/pdf/2204.01046.pdf">Style-Based Global Appearance Flow for Virtual Try-On</a>
  
</p>
<p><a name="A Curated">[3]</a> <a href="https://github.com/minar09/awesome-virtual-try-on">A Curated List of Awesome Virtual Try-on (VTON) Research </a>
</p>
<p><a name="Multi-Garment">[4]</a> <a href="https://virtualhumans.mpi-inf.mpg.de/papers/bhatnagar2019mgn/bhatnagar2019mgn.pdf">Multi-Garment:  Learning to Dress 3D People from Images</a>
</p>

<h3>Citatations</h3>
<p><a name="15">[1]</a> <a href="https://arxiv.org/abs/1608.01250?context=cs"
  >S. Yang, T. Ambert, Z. Pan, K. Wang, L. Yu, T. Berg, and M. C. Lin.</a>
  <em> Detailed garment recovery from a single-view
    image. In ICCV, 2017
  </em>
</p>
<p><a name="Paper doll parsing">[2]</a> <a href="http://www.tamaraberg.com/papers/paperdoll.pdf"
  >Yamaguchi, M. Hadi Kiapour, and T. L. Berg.</em></a>
  Paper doll parsing: Retrieving similar styles to parse clothing items. In ICCV, 2013.
  
</p>
<p><a name="Street-toshop">[3]</a> <a href="http://linliang.net/wp-content/uploads/2017/07/TMM_Clothes.pdf"
  >S. Liu, Z. Song, G. Liu, C. Xu, H. Lu, and S. Yan.</a>
  <em>Street-toshop: </em>Cross-scenario clothing retrieval via parts alignment and auxiliary set. In CVPR, 2012.
</p>

<p><a name="Collaborative fashion recommendation">[4]</a> <a href="https://dl.acm.org/doi/abs/10.1145/2733373.2806239"
  >Y. Hu, X. Yi, and L. S. Davis.</a>
  <em>Collaborative fashion recommendation</em> a functional tensor factorization approach. In
    ACM Multimedia, 2015.
</p>

<p><a name="Pixellevel domain transfer">[5]</a> <a href="https://arxiv.org/abs/1603.07442"
  >D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon.
  <em>
    In ECCV, 2016
  </em></a>
</p>
<p><a name="Generative Adversarial Nets">[6]</a> <a href="https://arxiv.org/pdf/1406.2661.pdf"
  >I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio</a>
  <em>Generative adversarial nets. In NIPS, 2014
  </em>
</p>
<p><a name="Conditional image synthesis">[6]</a> <a href="https://arxiv.org/abs/1610.09585"
  >A. Odena, C. Olah, and J. Shlens.</a>
  <em>Conditional image synthesis with auxiliary classifier gans. In ICML, 2017
  </em>
</p>

<p><a name="Learning fashion">[7]</a> <a href="https://arxiv.org/abs/1610.09585"
  >X. Han, Z. Wu, Y.-G. Jiang, and L. S. Davis.</a>
  <em> Learning fashion compatibility with bidirectional lstms. In ACM Multimedia, 2017.
  </em>
</p>
<p><a name="8">[8]</a> <a href="https://www.cs.utexas.edu/~grauman/papers/whittle-search-cvpr2012.pdf">
  A. Kovashka, D. Parikh, and K. Grauman.</a>
  <em> Whittlesearch:
    Image search with relative attribute feedback. In CVPR,
    2012.
  </em>
</p>
 
<p><a name="10">[10]</a> <a href="https://arxiv.org/abs/1703.10593"
  >J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros.</a>
  <em> Unpaired imageto-image translation using cycle-consistent adversarial networks. In ICCV, 2017
  </em>
</p>

<p><a name="11">[11]</a> <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Lassner_A_Generative_Model_ICCV_2017_paper.pdf"
  >C. Lassner, G. Pons-Moll, and P. V. Gehler.</a>
  <em> A generative model of people in clothing. In ICCV, 2017
  </em>
</p>

<p><a name="12">[12]</a> <a href="https://arxiv.org/abs/1710.07346"
  >C. S. Zhu, S. Fidler, R. Urtasun, D. Lin, and C. L. Chen. A generative.</a>
  <em> 
    Be your own prada: Fashion synthesis with structural coherence. In
    ICCV, 2017
  </em>
</p>
 

<p><a name="13">[13]</a> <a href="https://dl.acm.org/doi/10.1145/2185520.2185531"
  >P. Guan, L. Reiss, D. A. Hirshberg, A. Weiss, and M. J. Black</a>
  <em> Drape: Dressing any person. ACM TOG, 2012
  </em>
</p>



<p><a name="14">[14]</a> <a href="https://www.3dbodyscanning.org/cap/papers/2014/14406_14sekine.pdf"
  >M. Sekine, K. Sugita, F. Perbet, B. Stenger, and M. Nishiyama.</a>
  <em> Virtual fitting by single-shot body shape estimation. In 3D Body Scanning Technologies, 2014
  </em>
</p>
 

<p><a name="15">[15]</a> <a href="https://arxiv.org/abs/1608.01250?context=cs"
  >S. Yang, T. Ambert, Z. Pan, K. Wang, L. Yu, T. Berg, and M. C. Lin.</a>
  <em> Detailed garment recovery from a single-view
    image. In ICCV, 2017
  </em>
</p>
 

<p><a name="16">[16]</a> <a href="https://arxiv.org/abs/1709.04695"
  >N. Jetchev and U. Bergmann. </a>
  <em> The conditional analogy gan:
    Swapping fashion articles on people images. In ICCVW,
    2017.
  </em>
</p>


<h2>Team Members</h2>
                                                   
<p>Jivesh Poddar, Neha Cholera, Dhruvi Gajjar</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
